{\rtf1\ansi\uc1\deff13\deflang1024
{\fonttbl{\f0\fnil\fcharset0 Zapf Chancery;}
{\f1\fnil\fcharset204 Zapf Chancery;}
{\f2\fnil\fcharset204 Times;}
{\f3\fnil\fcharset204 Helvetica;}
{\f4\fnil\fcharset204 Helvetica;}
{\f5\fnil\fcharset204 Courier;}
{\f6\fnil\fcharset2 Symbol;}
{\f7\fnil\fcharset0 MT Extra;}
{\f8\fnil\fcharset238 Zapf Chancery;}
{\f9\fnil\fcharset238 Times;}
{\f10\fnil\fcharset238 Helvetica;}
{\f11\fnil\fcharset238 Helvetica;}
{\f12\fnil\fcharset238 Courier;}
{\f13\fnil\fcharset0 Times;}
{\f14\fnil\fcharset0 Helvetica;}
{\f15\fnil\fcharset0 Helvetica;}
{\f16\fnil\fcharset0 Courier;}
}
{\colortbl;
\red0\green0\blue0;
\red0\green0\blue255;
\red0\green255\blue255;
\red0\green255\blue0;
\red255\green0\blue255;
\red255\green0\blue0;
\red255\green255\blue0;
\red255\green255\blue255;
\red0\green0\blue128;
\red0\green128\blue128;
\red0\green128\blue0;
\red128\green0\blue128;
\red128\green0\blue0;
\red128\green128\blue0;
\red128\green128\blue128;
\red192\green192\blue192;
}
{\stylesheet
{\s0\fs20\snext0 Normal;}
{\s2\ql\sb240\sa60\keepn\f13\b\fs40 \sbasedon0\snext0 heading 1;}
{\s2\ql\sb240\sa60\keepn\f13\b\fs40\li0 \sbasedon0\snext0 heading 1;}
{\s1\ql\sb240\sa60\keepn\f13\b\fs40\li0 \sbasedon0\snext0 heading 1;}
{\s6\ql\sb240\sa60\keepn\f13\b\fs24\li2048 \sbasedon0\snext0 heading 5;}
{\s3\ql\sb240\sa60\keepn\f13\b\fs32\li512 \sbasedon0\snext0 heading 2;}
{\s7\ql\sb240\sa60\keepn\f13\b\fs24\li2560 \sbasedon0\snext0 heading 6;}
{\s4\ql\sb240\sa60\keepn\f13\b\fs32\li1024 \sbasedon0\snext0 heading 3;}
{\s5\ql\sb240\sa60\keepn\f13\b\fs24\li1536 \sbasedon0\snext0 heading 4;}
{\s6\ql\sb240\sa60\keepn\f13\b\fs24 \sbasedon0\snext0 heading 5;}
{\s1\qc\sb240\sa60\keepn\f13\b\fs40 \sbasedon0\snext0 part;}
{\s3\ql\sb240\sa60\keepn\f13\b\fs32 \sbasedon0\snext0 heading 2;}
{\s7\ql\sb240\sa60\keepn\f13\b\fs24 \sbasedon0\snext0 heading 6;}
{\s4\ql\sb240\sa60\keepn\f13\b\fs32 \sbasedon0\snext0 heading 3;}
{\s5\ql\sb240\sa60\keepn\f13\b\fs24 \sbasedon0\snext0 heading 4;}
}
{\info
{\title Original file was paper_phpFducEi.tex}
{\doccomm Created using latex2rtf 1.9.19 (released Nov 20 2007) on Tue Jun  4 22:35:46 2019
}
}
{\footer\pard\plain\f13\fs20\qc\chpgn\par}
\paperw12280\paperh15900\margl1842\margr1842\margt1908\margb2862\pgnstart0\widowctrl\qj\ftnbj\f13\aftnnar
{
\par\pard\qc {\fs30 \pard\qc\sl240\slmult1 \fi300 Time Series Forecasting with Deep Learning Models}
\par\pard\qc {\fs24 Ryan Silva, Eric Steen, Orion Darley \u8211? Stanford University }
\par\pard\qc {\fs24 }
\par\pard\qc {\fs24 May 2019}
\par\pard\qc {\fs24 }\par
{\pard\qj\sl240\slmult1 \fi0 \qc{\b Abstract}\par
\pard\qj\sl240\slmult1 \li1024\ri1024\fi0 Deep Learning models are increasingly used for a variety of time series forecasting applications and show significant promise as the industry-leading methodology for the foreseeable future, as previously used methods can be integrated into them easily. In this research, Deep Recurrent Neural Networks(RNN) with Long Short Term Memory(LSTM) are explored to handle the problem of uncertainty in Bitcoin(BTC) prices. We evaluate their efficacy in relation to more generally accepted time series approaches, in particular the (S)AR(I)MA(X) models. \par
}{\pard\qj\sl240\slmult1 \sb120 \fi300 {2} \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 1  Introduction\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 We apply deep learning to a duple of tactical parameters: lookback period and forecast length, hereby denoted as {{{\f6\'E1}{\i w}{\i l}{\i e}{\i n},{\i f}{\i l}{\i e}{\i n}{\f6\'F1}}} for window length and forecast length respectively. We execute a two-pronged strategy: {\par
\pard\qj\sl240\slmult1 \sb50 \li600\fi-300 1.\tab We use a discrete binary classification RNN model to determine the optimal hold period {{{\i f}{\i l}{\i e}{\i n}}} for a position held over a given {{{\i w}{\i l}{\i e}{\i n}}} randomly searched from a set of {{{\f6\'E1}{\i w}{\i l}{\i e}{\i n},{\i f}{\i l}{\i e}{\i n}{\f6\'F1}}} pairs in the range wlen in 1-480 days, flen in 1-20 days (swing trade range). \par
\pard\qj\sl240\slmult1 \sb50 \li600\fi-300 2.\tab We then use a continuous linear RNN model to predict the price following the optimal tactical parameters discovered in step 1. \par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Minimization of hold period is highly desirable for a trading program when risk & return characteristics are in parity across a set of tactical parameters as minimization reduces value-at-risk in aggregate. Having a sensible target price aids in tactical trading decisions such as risk-return calculations and exit point determination.\par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 2  Related Work\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Several papers are relevant to our work including the R2N2 paper 1\\s\\do5({\fs16 w})ebsite which covered x,y...\par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 3  Methods\par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 3.1  Dataset\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Our dataset consists of the following: {\par
\pard\qj\sl240\slmult1 \sb50 \li600\fi-300 1.\tab Bitcoin prices on a daily basis since 2012 \par
\pard\qj\sl240\slmult1 \sb50 \li600\fi-300 2.\tab Bitcoin prices on an hourly basis since july 2017 \par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s5\ql\sb240\sa60\keepn\f13\b\fs24 3.1.1  Scaling\par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Classification Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 We scale the both the price and volume series to adjust the range of values to be gaussian with zero mean and unit variance, without changing the distribution. This will prevent a feature with high variance that is orders of magnitude higher than other features from dominating and making the estimator unable to learn from other features correctly. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Linear Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 Ryan\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s5\ql\sb240\sa60\keepn\f13\b\fs24 3.1.2  Regularization\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 We regard the old wall street maxim "The trend is your friend" to be a good starting point for the evaluation of deep learning on exchange traded financial assets. So in order to reduce noise that might prevent the machine learning algorithm from learning the trend, we regularize both the classification and linear model series data using a Kalman filter with the {\f16 \field{\*\fldinst{ HYPERLINK "https://pykalman.github.io/" }{{}}}{\fldrslt{pykalman}}} library.\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Kalman Filter\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 The Kalman filter process has two steps: {\par
\pard\qj\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab The prediction step, which uses a previously estimated state and the linear model to predict the value of the next state as well as the states estimated covariance.\par
\pard\qj\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab The update step, which uses the current output together with the statistical properties of the model, to correct the state estimate. The values calculated are the innovation covariance, the Kalman gain resulting in the updated state estimate and state estimate covariance. \par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s5\ql\sb240\sa60\keepn\f13\b\fs24 3.1.3  Preprocessing\par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Classification Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 Eric TODO (windowing and balancing the data) 4\\s\\do5({\fs16 w})ebsite \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Linear Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 Ryan TODO\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 3.2  Feature Engineering\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 We explore a variety of data preprocessing and augmentation techniques gathered from the literature, and implement them as part of our EDA (Exploratory Data Analysis). Our EDA includes the following: \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Classification Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 Price and Volume were used in the classification model for simplicity to ensure model sensitivity to the most widely distributed data features in the financial asset space.\par
\pard\qj\sl240\slmult1 \fi300 {Everything should be made as simple as possible, but not simpler. } - Albert Einstein\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Linear Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 Ryan TODO (ta library) 3\\s\\do5({\fs16 w})ebsite.\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 3.3  Activation, Loss\par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Classification Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 Eric TODO (windowing and balancing the data) 4\\s\\do5({\fs16 w})ebsite \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Linear Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 Ryan TODO (ta library) 3\\s\\do5({\fs16 w})ebsite.\par
\pard\qj\sl240\slmult1 \fi300 We plan to use the root mean squared error (RMSE) between predicted and true financial time series as the cost function and evaluation metric for our models, as implemented in the EDA notebook. We also plan to build out a data pipeline using simple data stores (redis or sqlite).\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 3.4  Hyperparameter Tuning\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 The hyperparameter decisions that were most effective at tuning the model were smaller number of hidden units per layer, dropout, scaling and normalizing the data in the pre-processing step, learning rate, and batch normalization. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s5\ql\sb240\sa60\keepn\f13\b\fs24 3.4.1  Batch Size\par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Classification Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 Due to the \rquote small data\rquote  nature of our inquiry, a lower batch size of 64 for the binary model sufficed. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Linear Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 A batch size of 32 was adequate for the linear model. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s5\ql\sb240\sa60\keepn\f13\b\fs24 3.4.2  Learning Rate\par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Classification Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 Many learning rates were tried in the range .1 - .00001 with .001 being preferable \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Linear Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 .001 \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s5\ql\sb240\sa60\keepn\f13\b\fs24 3.4.3  Dropout\par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Classification Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 A dropout ratio of 0.4 improved the volatility of the classification model significantly from baseline (sans any regularization). \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Linear Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 The linear model (Ryan)... \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s5\ql\sb240\sa60\keepn\f13\b\fs24 3.4.4  Hidden Units\par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Classification Model\par
} \pard\qj\sl240\slmult1 \sb60 \fi0 Given the \rquote small data\rquote  nature of our inquiry, a lower number of units was most effective for increasing accuracy, reducing loss, and decreasing overall volatility of metrics. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s6\ql\sb240\sa60\keepn\f13\b\fs24 Linear Model\par
} \pard\qj\sl240\slmult1 \sb180 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 3.5  Tactical Parameters\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Our research showed a 120 period window with a forecast period of 3 gave the greatest accuracy with the lowest loss and the least volatility of all other tested combinations, taking into account risk-parity considerations (4 periods showed very similar results, with slightly more stability in accuracy across epochs, but 3 gives less value-at-risk in practical application in finance).\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 3.6  Results\par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s5\ql\sb240\sa60\keepn\f13\b\fs24 3.6.1  Classification Model\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Our supervised learning approach indicates that a 120 period window with a forecast period of 3 was the optimal random swing trade for bitcoin since 10/11/2015. While this may change in the future, we are optimistic that additional data and feature engineering, novel approaches, and additional human insights will enhance the trading tactics disclosed herein. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s5\ql\sb240\sa60\keepn\f13\b\fs24 3.6.2  Linear Prediction\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Ryan\par
{\pard\qj\sl240\slmult1 \sb240 \fi300  \par
\pard\qc\sl240\slmult1 \fi0  \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_download}1{\*\bkmkend BMfig_download}: Our Results}{\field{\*\fldinst TC "1 Our Results" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb480 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 4  Conclusions\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Deep Recurrent Neural Networks are effective in tactical trading decision making support . \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 5  Future Work\par
}\pard\qj\sl240\slmult1 \sb300 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 6  Code\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Our code is at {\f16 \field{\*\fldinst{ HYPERLINK "https://github.com/ericsteen/crypto_data" }{{}}}{\fldrslt{https://github.com/ericsteen/crypto_data}}} The code of particular interest is in {\b0\i0\scaps0\f16 rnn.py  and \\verb data}experiments.ipynb. Data gathering scripts are located in /lib as well.\par
}}}
