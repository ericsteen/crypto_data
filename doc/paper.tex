\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Time Series Forecasting with Deep Learning Models}
\author{Eric Steen, Orion Darley, Ryan Silva – Stanford University }
\date{May 2019}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\begin{document}

\maketitle

\section{Introduction}
The purpose of this research is to benchmark the effectiveness of deep learning neural networks on price forecasting of bitcoin. Long-Short Term Memory (LSTM) and Recurrent Neural Networks (RNN) are explored due to their cutting-edge effectiveness. We integrate (S)AR(I)MA(X) models into our RNN model.

\paragraph{}
Research questions initially posed include:
\begin{enumerate}
    \item Can Deep Neural Nets outperform traditional time series models in forecasting price? If not, what, if any, advantages would the NNet approach provide?
    \item How do hyperparameters affect the predictive ability of the LSTM and RNNs? Are deeper nets able to achieve better performance than shallow ones, or are one to two hidden layers a more effective approach for this type of task? What is an appropriate dropout rate for our task?
\end{enumerate}

\paragraph{}
Our team experimented with several LSTM models and are still deciding a model strategy. The following steps are taken to measure the predicted values distance from ground truth:

\begin{enumerate}
\item Import bitcoin pricing from 2012-current, to include open price, close price, high and low prices, volume transacted, and percentage change over a daily interval.
\item Plotted the distributions of each and determined the threshold for the training data. Most of the data is heavily skewed as to where past price changes and autocorrelation do not necessarily represent the current trend nor the near-future trend of Bitcoin prices.
\item Scaled the data and created the training and test structure.
\item Built four models using various parameters (various activation functions, hidden nodes, dropout rates, batch sizes, etc.)
\item Benchmarked the predicted values for four models versus the actuals.
\end{enumerate}
\paragraph{}
The results are fairly similar, however a more focused strategy for deep learning in time series needs to be developed (see results below).

%%\includegraphics[width=9cm]{download}

\section{Deep Recurrent Neural Nets for Time Series}
\paragraph{We use RNN's to predict bitcoin prices.}

\subsection{Dataset}
\paragraph{}
Our dataset consists of the following:
\begin{enumerate}
    \item Bitcoin prices on a daily basis since 2012
    \item Bitcoin prices on an hourly basis since july 2017
\end{enumerate}

\subsubsection{Regularization}
\paragraph{}
We regard the old wall street maxim 'The trend is your friend' to be a good starting point for the evaluation of deep learning on financial time series, so to reduce noise that might prevent the machine learning algorithm from learning the trend, we will explore regularizing the data with wavelet transformation using the \href{https://pywavelets.readthedocs.io/en/latest/#main-features}{PyWavelets library.}

\paragraph{}
The process to regularize is as follows:
\begin{enumerate}
\item The data is transformed using a Wavelet transform.
\item Coefficients more than 1 standard deviation away from the mean of coefficients are removed.
\item Inverse transform the new coefficients to get the denoised data.
\end{enumerate}

\paragraph{}@TechReport{•,
author = {•},
title = {•},
institution = {•},
year = {•},
OPTkey = {•},
OPTtype = {•},
OPTnumber = {•},
OPTaddress = {•},
OPTmonth = {•},
OPTnote = {•},
OPTannote = {•}
}
Here is the integral form of the wavelet transformation used by the \textit{PyWavelets} library:
\[\left[W_\psi f\right](a, b) = \frac{1}{\sqrt{|a|}} \int_{-\infty}^\infty \overline{\psi\left(\frac{x-b}{a}\right)}f(x)dx\]

\subsection{Feature Engineering}

\paragraph{}
We explore a variety of data preprocessing and augmentation techniques gathered from the literature, and implement them as part of our EDA (Exploratory Data Analysis). Our EDA includes the following:

\begin{enumerate}
\item  We draw inspiration from the R2N2 \cite{1_website} algorithm, which uses the VAR and VARMAX algorithms to pre-process the time series data and produce residuals, which are used as input to an LSTM. We show that the VAR model performs quite well in one day forecasts. We can use these techniques to augment our data, providing both the raw features, as well as VAR and/or VARMAX residuals.

\item Found some success by pre-processing the data using filters, namely the Kalman and Wavelet filters \cite{2_website}.

\item 3 \cite{3_website} gives multiple technical indicators for data augmentation including momentum, trend, volatility, and volume. These can be calculated with the python ‘TA’ library . These features can be used to further augment the LSTM input.

\item 4 \cite{4_website} proposes the technique of segmenting the time series into a number of consecutive 'windows'  , and a label which is the next window of data. Preprocessing the data into this format and saving to disk can save a lot of computation time each run. There are also many hyperparameters here, such as the size of the window, how many windows to include in a training example, if and how much he windows overlap, etc.
\end{enumerate}

\paragraph{}
We plan to use the root mean squared error (RMSE) between predicted and true financial time series as the cost function and evaluation metric for our models, as implemented in the EDA notebook. We also plan to build out a data pipeline using simple data stores (redis or sqlite).

\section{Code}
Our code is at \url{https://github.com/ericsteen/crypto_data}
The code of particular interest is in \verb btc_lstm_final_v2.ipynb  and \verb data_experiments.ipynb. Data gathering scripts are located in /lib as well.

\bibliography{references}

% \subsection{Hyperparameters}
% \begin{enumerate}
%     \item Layers
%     \item alpha
%     \item gamma
%     \item dropout
% \end{enumerate}

% \subsection{Our Model – Deep Recurrent Neural Nets with LSTM}
% % \begin{figure}[h!]
% % \centering
% % \includegraphics[scale=1.7]{universe}
% % \caption{The Universe}
% % \label{fig:universe}
% % \end{figure}

% \section{Conclusion}
% ``Time Series, in conjunction with deep neural nets is a win, win proposition for quantitative analysis of cryptoassets.'' \citep{adams1995hitchhiker}

\bibliographystyle{plain}

\end{document}
