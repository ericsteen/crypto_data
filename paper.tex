\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Time Series Forecasting with Deep Learning Models}
\author{Eric Steen, Orion Darley, Ryan Silva – Stanford University }
\date{May 2019}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\begin{document}

\maketitle

\section{Introduction}
The purpose of this research is to benchmark the effectiveness of deep learning neural networks on price forecasting of bitcoin. Long-Short Term Memory (LSTM) and Recurrent Neural Networks (RNN) are explored due to their cutting-edge effectiveness.

\paragraph{}
Research questions initially posed include:
\begin{enumerate}
    \item Can Deep Neural Nets outperform traditional time series models in forecasting price? If not, what, if any, advantages would the NNet approach provide?
    \item How do hyperparameters affect the predictive ability of the LSTM and RNNs? Are deeper nets able to achieve better performance than shallow ones, or are one to two hidden layers a more effective approach for this type of task? What is an appropriate dropout rate for our task?
\end{enumerate}

\paragraph{}
Our team experimented with several LSTM models and are still deciding a model strategy. The following steps are taken to measure the predicted values distance from ground truth:

\begin{enumerate}
\item Import bitcoin pricing from 2012-current, to include open price, close price, high and low prices, volume transacted, and percentage change over a daily interval.
\item Plotted the distributions of each and determined the threshold for the training data. Most of the data is heavily skewed as to where past price changes and autocorrelation do not necessarily represent the current trend nor the near-future trend of Bitcoin trading.
\item Scaled the data and created the training and test structure.
\item Built four models using various parameters (various activation functions, hidden nodes, dropout rates, batch sizes, etc.)
\item Benchmarked the predicted values for four models versus the actuals.
\end{enumerate}
\paragraph{}
The results are fairly similar, however a more focused strategy for deep learning in time series needs to be developed (see results below).
\section{Traditional Approaches}
\paragraph{}
Before the deep learning era, mathematical models and approaches for time series and signal processing included the following:

\begin{enumerate}
    \item Time/Frequency domain analysis: how the series evolves in relation to time/amplitude; For the frequency domain. Signal Processing, Fourier analysis and Wavelet Transforms are common.
    \item Cluster Analysis: 'Nearest Neighbors' accounts for signals of differing length, where the notion of similarity breaks down.
    \item (S)AR(I)MA(X) models: very popular; based on linear self-dependence inside of time series (autocorrelation) which can be used to explain future fluctuations.
    \item Decomposition: divide the series into logical parts that can be summed or multiplied to obtain the initial time series: trend part, seasonal part, and residuals.
    \item Nonlinear dynamics: differential equations (ordinary, partial, stochastic, etc.) for modeling dynamical systems that are in fact signals or time series.
    \item Machine learning: The subject of this paper; Can include any or all of the above. There is great potential for deep learning models to help determine feasibility along a vast swath of dimensions.
\end{enumerate}

\section{Deep Learning Approach}
\paragraph{We use Deep Recurrent Neural Networks to x, y, z.}

\subsection{Dataset}
\paragraph{}
Our dataset consists of the following:
\begin{enumerate}
    \item Bitcoin prices on a daily basis from x1 to x2
    \item Bitcoin prices on an hourly basis from x1 to x2
\end{enumerate}

\subsection{Regularization}
\paragraph{}
We regard the old wall street maxim 'The trend is your friend' to be a good starting point for the evaluation of deep learning on financial time series, so to reduce noise that might prevent the machine learning algorithm from learning the trend and structure, we will explore regularizing the data with wavelet transformation using the \href{https://pywavelets.readthedocs.io/en/latest/#main-features}{PyWavelets library.}

\paragraph{}
The process is as follows:
\begin{enumerate}
\item The data is transformed using Wavelet transform.
\item Coefficients more than 1 standard deviation away from the mean of coefficients are removed.
\item Inverse transform the new coefficients to get the denoised data.
\end{enumerate}

\item Here is the integral form of wavelet transformation to denoise time series data:
\[\left[W_\psi f\right](a, b) = \frac{1}{\sqrt{|a|}} \int_{-\infty}^\infty \overline{\psi\left(\frac{x-b}{a}\right)}f(x)dx\]

\subsection{Feature Engineering}

\paragraph{}
Many of the traditional approaches described above can be used for feature engineering and data augmentation. We explore a variety of data preprocessing and augmentation techniques we gathered from the literature, and implement them as part of our EDA (Exploratory Data Analysis). Our EDA includes the following:

\begin{enumerate}
\item  We draw inspiration from the R2N2 \cite{1_website} algorithm, which uses the VAR and VARMAX algorithms to pre-process the time series data and produce residuals, which are used as input to an LSTM. We show that the VAR model, using the features ‘Price’ (closing value), ‘Open’, ‘High’, ‘Low’, ‘Volume’, and ‘Change \%’, performs quite well in one day forecasts. We can use these techniques to augment our data, providing both the raw features, as well as VAR and/or VARMAX residuals.

\item Found some success by pre-processing the data using filters, namely the Kalman and Wavelet filters \cite{2_website}. They used the pyKalman and pyWavelet libraries to produce a denoised version of the time series and input this into the model. We can potentially use this preprocessing method as another feature into our Deep Learning Models.

\item Gives multiple technical indicators for data augmentation including momentum, trend, volatility, and volume. These can be calculated with the python ‘TA’ library \cite{3_website}, and we have examples in our code repository on our bitcoin dataset. These features can be used to further augment the LSTM input.

\item Proposes the technique of segmenting the time series into a number of consecutive 'windows'  \cite{4_website}, and a label which is the next window of data. Preprocessing the data into this format and saving to disk can save a lot of computation time each run. There are also many hyperparameters here, such as the size of the window, how many windows to include in a training example, if and how much he windows overlap, etc.
\end{enumerate}

\paragraph{}
We plan to use the root mean squared error (RMSE) between predicted and true financial time series as the cost function and evaluation metric for our models, as implemented in the EDA notebook. We also plan to build out a data pipeline using simple data stores (redis or sqlite).

\subsection{Hyperparameters}
\begin{enumerate}
    \item Layers
    \item alpha
    \item gamma
    \item dropout
\end{enumerate}

\subsection{Our Model – Deep Recurrent Neural Nets with LSTM}
% \begin{figure}[h!]
% \centering
% \includegraphics[scale=1.7]{universe}
% \caption{The Universe}
% \label{fig:universe}
% \end{figure}

\section{Conclusion}
``Time Series, in conjunction with deep neural nets is a win, win proposition for quantitative analysis of cryptoassets.'' \citep{adams1995hitchhiker}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
